services:
  inbox-assistant:
    build:
      context: ..
      dockerfile: aech-rt-inbox-assistant/Dockerfile
    image: aech-rt-inbox-assistant
    env_file:
      - .env
    volumes:
      # AECH_HOST_DATA: path to aech-main/data on host (set in .env)
      - ${AECH_HOST_DATA}/users/${DELEGATED_USER}:/home/agentaech
      # RT trigger queue root (outbox/processing/done/failed/dedupe)
      - ${AECH_HOST_DATA}/rt_triggers/inbox-assistant:/triggers:rw
      # Token cache (explicit for clarity)
      - ${AECH_HOST_DATA}/app_context/aech-cli-msgraph/token_cache.json:/app/token_cache.json
    restart: unless-stopped
    environment:
      - PYTHONUNBUFFERED=1
      - OPENAI_API_KEY
      - MSGRAPH_TOKEN
      - DELEGATED_USER
      - POLL_INTERVAL
      - MODEL_NAME
      - AZURE_CLIENT_ID
      - AZURE_TENANT_ID
      - MSGRAPH_TOKEN_CACHE_PATH=/app/token_cache.json

  # One-shot backfill: docker compose run --rm backfill
  # For GPU on Nvidia: requires nvidia-container-toolkit installed on host
  backfill:
    image: aech-rt-inbox-assistant
    env_file:
      - .env
    volumes:
      - ${AECH_HOST_DATA}/users/${DELEGATED_USER}:/home/agentaech
      - ${AECH_HOST_DATA}/app_context/aech-cli-msgraph/token_cache.json:/app/token_cache.json
      # Shared HuggingFace cache (embedding models ~2GB, don't download per-user)
      - ${AECH_HOST_DATA}/app_context/huggingface:/home/agentaech/.cache/huggingface
    environment:
      - PYTHONUNBUFFERED=1
      - OPENAI_API_KEY
      - DELEGATED_USER
      - AZURE_CLIENT_ID
      - AZURE_TENANT_ID
      - MSGRAPH_TOKEN_CACHE_PATH=/app/token_cache.json
      # Per-task model configuration
      - CLASSIFICATION_MODEL=${CLASSIFICATION_MODEL:-openai-responses:gpt-5-nano}
      - WM_MODEL=${WM_MODEL:-openai-responses:gpt-5-mini}
      # Embedding model (local, no API cost)
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-BAAI/bge-m3}
      - EMBEDDING_BATCH_SIZE=${EMBEDDING_BATCH_SIZE:-32}
    # bge-m3 needs ~4GB RAM; ensure Docker Desktop has 8GB+ memory allocated
    entrypoint: ["/app/scripts/backfill.sh"]
    restart: "no"

  # Interactive shell for testing individual commands
  # Usage: docker compose run --rm cli aech-cli-inbox-assistant extract-content --limit 10 --human
  cli:
    image: aech-rt-inbox-assistant
    env_file:
      - .env
    volumes:
      - ${AECH_HOST_DATA}/users/${DELEGATED_USER}:/home/agentaech
      - ${AECH_HOST_DATA}/app_context/aech-cli-msgraph/token_cache.json:/app/token_cache.json
      - ${AECH_HOST_DATA}/app_context/huggingface:/home/agentaech/.cache/huggingface
    environment:
      - PYTHONUNBUFFERED=1
      - OPENAI_API_KEY
      - DELEGATED_USER
      - AZURE_CLIENT_ID
      - AZURE_TENANT_ID
      - MSGRAPH_TOKEN_CACHE_PATH=/app/token_cache.json
      - CLASSIFICATION_MODEL=${CLASSIFICATION_MODEL:-openai-responses:gpt-5-nano}
      - WM_MODEL=${WM_MODEL:-openai-responses:gpt-5-mini}
    entrypoint: []
    command: ["bash"]
    restart: "no"

  # GPU-enabled backfill for Nvidia hosts (GB10, etc.)
  # Usage: docker compose run --rm backfill-gpu
  backfill-gpu:
    image: aech-rt-inbox-assistant
    env_file:
      - .env
    volumes:
      - ${AECH_HOST_DATA}/users/${DELEGATED_USER}:/home/agentaech
      - ${AECH_HOST_DATA}/app_context/aech-cli-msgraph/token_cache.json:/app/token_cache.json
      - ${AECH_HOST_DATA}/app_context/huggingface:/home/agentaech/.cache/huggingface
    environment:
      - PYTHONUNBUFFERED=1
      - OPENAI_API_KEY
      - DELEGATED_USER
      - AZURE_CLIENT_ID
      - AZURE_TENANT_ID
      - MSGRAPH_TOKEN_CACHE_PATH=/app/token_cache.json
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-BAAI/bge-m3}
      - EMBEDDING_BATCH_SIZE=${EMBEDDING_BATCH_SIZE:-32}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    entrypoint: ["/app/scripts/backfill.sh"]
    restart: "no"
